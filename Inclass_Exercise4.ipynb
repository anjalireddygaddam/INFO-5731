{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inclass_Exercise4",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1zKx31KgsYKCcb0ovcXhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjalireddygaddam/INFO-5731/blob/main/Inclass_Exercise4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGUi2prsOt-p",
        "outputId": "6c95375e-10be-4eea-f647-89fd834fa4b1"
      },
      "source": [
        "#pip install nltk\r\n",
        "#1.1\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import glob\r\n",
        "import re\r\n",
        "import csv\r\n",
        "import nltk\r\n",
        "import string\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "\r\n",
        "file = open('/content/drive/MyDrive/01-05-1  Adams v Tanner.txt') \r\n",
        "\r\n",
        "sentenceCount = 0\r\n",
        "wordCount = 0\r\n",
        "char_count = 0\r\n",
        "total_char=0\r\n",
        "specialCharCount=0\r\n",
        "numberCount =0\r\n",
        "countOfStopWords =0\r\n",
        "upperCaseCount =0\r\n",
        "listOfStopWords=stopwords.words('english')\r\n",
        "Lines = file.readlines() \r\n",
        "for line in Lines:\r\n",
        "  sent = line.split(\".\")\r\n",
        "  for sentence in sent:\r\n",
        "    if len(sentence)>0:\r\n",
        "      filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\r\n",
        "      words_without_punc = [word for word in filtered.split() if word]\r\n",
        "      total_char = total_char + sum(map(len, words_without_punc))\r\n",
        "      wordCount = wordCount + sum([i.strip(string.punctuation).isalnum() for i in sentence.split()])\r\n",
        "      sentenceCount = sentenceCount+1\r\n",
        "      words = sentence.split(\" \")\r\n",
        "      for w in words:\r\n",
        "        new = re.sub('[\\w]+' ,'', w)\r\n",
        "        specialCharCount = specialCharCount + len(new)\r\n",
        "        if w.isnumeric()==True:\r\n",
        "          numberCount = numberCount+1\r\n",
        "        if w in listOfStopWords:\r\n",
        "          countOfStopWords = countOfStopWords + 1\r\n",
        "        if w.isupper()==True:\r\n",
        "          upperCaseCount=upperCaseCount+1\r\n",
        "file = open('/content/drive/MyDrive/01-05-1  Adams v Tanner.txt') \r\n",
        "char_count = len(file.read())\r\n",
        "print(\"count of sentences=\", sentenceCount)\r\n",
        "print(\"count of words=\", wordCount)\r\n",
        "print(\"count of characters=\", char_count)\r\n",
        "print(\"average length of words is=\", total_char/wordCount)\r\n",
        "print(\"count of special characters is=\", specialCharCount)\r\n",
        "print(\"number of numerics=\", numberCount)\r\n",
        "print(\"stop word count =\", countOfStopWords)\r\n",
        "print(\"count of upper case words =\", upperCaseCount)\r\n",
        "#1.2\r\n",
        "import nltk\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import pandas as pd\r\n",
        "from textblob import Word\r\n",
        "from textblob import TextBlob\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "content= open('/content/drive/MyDrive/01-05-1  Adams v Tanner.txt').read()\r\n",
        "sentences=nltk.tokenize.sent_tokenize(content)\r\n",
        "Data={'sentences':sentences}\r\n",
        "f=pd.DataFrame(data=Data)\r\n",
        "\r\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "ans1=f['sentences'].head()\r\n",
        "f['sentences'] = f['sentences'].str.replace('[^\\w\\s]','')\r\n",
        "ans2=f['sentences'].head()\r\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in listOfStopWords))\r\n",
        "ans3=f['sentences'].head()\r\n",
        "Frequent = pd.Series(' '.join(f['sentences']).split()).value_counts()[:10]\r\n",
        "Frequent=list(Frequent.index)\r\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in Frequent))\r\n",
        "ans4=f['sentences'].head()\r\n",
        "rare = pd.Series(' '.join(f['sentences']).split()).value_counts()[-10:]\r\n",
        "rare=list(rare.index)\r\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\r\n",
        "ans5=f['sentences'].head()\r\n",
        "f['sentences'][:5].apply(lambda x: str(TextBlob(x).correct()))\r\n",
        "ans6=f['sentences'].head()\r\n",
        "ans7=TextBlob(f['sentences'][1]).words\r\n",
        "stem = PorterStemmer()\r\n",
        "ans8=f['sentences'][:5].apply(lambda x: \" \".join([stem.stem(word) for word in x.split()]))\r\n",
        "f['sentences'] = f['sentences'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "ans9=f['sentences'].head()\r\n",
        "print(\"Lower casing \",ans1)\r\n",
        "print(\"Punctuation removal \",ans2)\r\n",
        "print(\"Stopwords removal\", ans3)\r\n",
        "print(\"Frequent words removal\", ans4)\r\n",
        "print(\"Rare words removal \",ans5)\r\n",
        "print(\"Spelling correction \",ans6)\r\n",
        "print(\"Tokenization \",ans7)\r\n",
        "print(\"Stemming \",ans8)\r\n",
        "print(\"Lemmatization\", ans9)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#1.3\r\n",
        "f.to_csv('output_clean.csv',index=False)\r\n",
        "\r\n",
        "\r\n",
        "#1.4\r\n",
        "frequency = pd.Series(' '.join(f['sentences']).split()).value_counts()\r\n",
        "print(\"term frequency=\",frequency)\r\n",
        "TextBlob(f['sentences'][0]).ngrams(1)\r\n",
        "TextBlob(f['sentences'][0]).ngrams(2)\r\n",
        "TextBlob(f['sentences'][0]).ngrams(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "count of sentences= 442\n",
            "count of words= 3654\n",
            "count of characters= 20454\n",
            "average length of words is= 4.386699507389163\n",
            "count of special characters is= 686\n",
            "number of numerics= 64\n",
            "stop word count = 1676\n",
            "count of upper case words = 92\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Lower casing  0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Punctuation removal  0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Stopwords removal 0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Frequent words removal 0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Rare words removal  0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Spelling correction  0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "Tokenization  ['adams', 'tanner', 'horton']\n",
            "Stemming  0                             5 ala 740 suprem alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                    synopsi writ error circuit sumter\n",
            "4    west headnot 2 1 chattel mortgag crop grow exi...\n",
            "Name: sentences, dtype: object\n",
            "Lemmatization 0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n",
            "term frequency= rep           16\n",
            "growing       16\n",
            "contract      16\n",
            "plaintiff     15\n",
            "ala           15\n",
            "              ..\n",
            "tuskaloosa     1\n",
            "making         1\n",
            "instructed     1\n",
            "cost           1\n",
            "mr             1\n",
            "Length: 782, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5', 'ala', '740']),\n",
              " WordList(['ala', '740', 'supreme']),\n",
              " WordList(['740', 'supreme', 'alabama'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYvA8qpRQDgE",
        "outputId": "0b7c5260-46ff-4227-9b8d-5b4a2138fcdd"
      },
      "source": [
        "#Write a Python program to remove leading zeros from an IP address.\r\n",
        "#ip = \"260.08.094.109\"\r\n",
        "\r\n",
        "# Write your code here\r\n",
        "\r\n",
        "ip = \"260.08.094.109\"\r\n",
        "ips = ip.split(\".\")\r\n",
        "count =0\r\n",
        "new_ip = \"\"\r\n",
        "for element in ips:\r\n",
        "  if ips[0]=='0':\r\n",
        "    element = element[1:]\r\n",
        "  if count < len(ips)-1:\r\n",
        "    new_ip = new_ip + element + \".\"\r\n",
        "  else:\r\n",
        "    new_ip = new_ip + element\r\n",
        "  count=count+1\r\n",
        "print(new_ip)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.08.094.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFFBfEN0QrLb",
        "outputId": "ef47c0e3-80ff-466b-de75-5bbc20ead5f1"
      },
      "source": [
        "# Write a Python Program to extract all the years from the following sentence. \r\n",
        "# Write your code here\r\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "listOfWords = sentence.split(\" \")\r\n",
        "ans = []\r\n",
        "for element in listOfWords:\r\n",
        "  if element[0].isnumeric() == True:\r\n",
        "    if element[0]=='2':\r\n",
        "      word = \"\";\r\n",
        "      for character in element:\r\n",
        "        if character.isnumeric() == True:\r\n",
        "          word = word + character\r\n",
        "      ans.append(word)\r\n",
        "for year in ans:\r\n",
        "  print(year)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2010\n",
            "2010\n",
            "2019\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}